{"subject": "machine_learning", "question": "Question: Assume we have N training samples $(x_1, y_1), \\ldots, (x_N, y_N)$ where for each sample $i \\in \\{1, \\ldots, N\\}$ we have that $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}$. We want to classify the dataset using the exponential loss $L(w) = \\frac{1}{N} \\sum_{i=1}^N \\exp(-y_i x_i^T w)$ for $w \\in \\mathbb{R}^d$. Which of the following statements is true?\n\nOptions:\nA. The loss function $L$ is non-convex in $w$.\nB. There exists a vector $w^*$ such that $L(w^*) = 0$.\nC. If I find a vector $w^*$ such that $L(w^*) < \\frac{1}{N}$, then $w^*$ linearly separates my dataset.\nD. None of the statements are true.\nE. This corresponds to doing logistic regression as seen in class.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Assume we have N training samples $(x_1, y_1), \\ldots, (x_N, y_N)$ where for each sample $i \\in \\{1, \\ldots, N\\}$ we have that $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$. For $\\lambda \\geq 0$, we consider the following loss: $L_\\lambda(w) = \\frac{1}{N} \\sum_{i=1}^N (y_i - x_i^T w)^2 + \\lambda \\|w\\|_2$, and let $C_\\lambda = \\min_{w \\in \\mathbb{R}^d} L_\\lambda(w)$ denote the optimal loss value. Which of the following statements is true?\n\nOptions:\nA. $C_\\lambda$ is a non-increasing function of $\\lambda$.\nB. For $\\lambda = 0$, the loss $L_0$ is convex and has a unique minimizer.\nC. None of the statements are true.\nD. $C_\\lambda$ is a non-decreasing function of $\\lambda$.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Consider the logistic regression loss $L : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ for a binary classification task with data $(x_i, y_i) \\in \\mathbb{R}^d \\times \\{0, 1\\}$ for $i \\in \\{1, \\ldots N\\}$:\n$L(w) = \\frac{1}{N} \\sum_{i=1}^N (\\log(1 + e^{x_i^T w}) - y_i x_i^T w)$. Which of the following is a gradient of the loss $L$?\n\nOptions:\nA. $\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N (x_i \\frac{e^{x_i^T w}}{1 + e^{x_i^T w}} - y_i x_i^T w)$\nB. $\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N x_i (y_i - \\frac{e^{x_i^T w}}{1 + e^{x_i^T w}})$\nC. $\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N (\\frac{e^{x_i^T w}}{1 + e^{x_i^T w}} - y_i x_i)$\nD. $\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^N x_i (\\frac{1}{1 + e^{-x_i^T w}} - y_i)$\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Consider the loss function $L : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, $L(w) = \\frac{\\beta}{2} \\|w\\|^2$, where $\\beta > 0$ is a constant. We run gradient descent on $L$ with a stepsize $\\gamma > 0$ starting from some $w_0 \\neq 0$. Which of the statements below is true?\n\nOptions:\nA. Gradient descent converges in two steps for $\\gamma = \\frac{1}{\\beta}$ (i.e., $w_2$ is the first iterate attaining the global minimum of $L$).\nB. Gradient descent with stepsize $\\gamma = \\frac{2}{\\beta}$ produces iterates that diverge to infinity ($\\|w_t\\| \\rightarrow \\infty$ as $t \\rightarrow \\infty$).\nC. Gradient descent converges to the global minimum for any stepsize $\\gamma > 0$.\nD. Gradient descent converges to the global minimum for any stepsize in the interval $\\gamma \\in (0, \\frac{2}{\\beta})$.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: LASSO is a linear regression with $L_1$ regularization, it is equivalent to imposing on the weights a:\n\nOptions:\nA. Laplace prior.\nB. Logistic prior.\nC. Gaussian prior.\nD. None of the other options.\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: How does the bias-variance decomposition of a ridge regression estimator compare with that of the ordinary least-squares estimator in general?\n\nOptions:\nA. Ridge has a smaller bias, and smaller variance.\nB. Ridge has a larger bias, and larger variance.\nC. Ridge has a larger bias, and smaller variance.\nD. Ridge has a smaller bias, and larger variance.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Our task is to classify whether an animal is a dog (class 0) or a cat (class 1) based on the following features: $x_1$: height, $x_2$: length of whiskers, $x_3$: thickness of fur. We perform standard normal scaling on the training features so that they have a mean of zero and standard deviation of 1. We have trained a Logistic Regression model to determine the probability that the animal is a cat, $p(1|x,w)$. Our classifier learns that cats have a lower height and longer whiskers than dogs, while the thickness of fur is not relevant to the classification outcome. Which of the following is true about the weights $w$ learned by the classifier?\n\nOptions:\nA. $w_1 < w_2 < w_3$\nB. $w_2 < w_3 < w_1$\nC. $w_3 < w_2 < w_1$\nD. $w_1 < w_3 < w_2$\nE. $w_2 < w_1 < w_3$\nF. $w_3 < w_1 < w_2$\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Let's imagine that every pet owner in Europe sends us their dog and cat data to train our model, so we have lots of training data. Which of the following is true about the possible optimizers used to train our model? We assume that the optimal hyperparameters are used for each optimizer.\n\nOptions:\nA. Gradient descent takes fewer steps to converge, while Newton steps are more computationally efficient.\nB. Overall, gradient descent and Newton's method are equal in terms of computational complexity.\nC. Gradient descent steps are more computationally efficient, while Newton's method takes fewer steps to converge.\nD. Gradient descent and Newton's method take similar number of iterations to converge.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Recall that the hard-margin SVM problem corresponds to: $\\min_{w \\in \\mathbb{R}^d}, \\forall i: y_i w^T x_i \\geq 1 \\|w\\|^2$. Now consider the 2-dimensional classification dataset corresponding to the 3 following datapoints: $x_1 = (-1, 2)$, $x_2 = (1, 2)$, $x_3 = (0, -2)$ and $y_1 = y_2 = 1$, $y_3 = -1$. Which of the following statements is true:\n\nOptions:\nA. Our dataset is not linearly separable and hence it does not make sense to consider the hard-margin problem.\nB. The unique vector which solves the hard-margin problem for our dataset is $w^* = (0, 1)$.\nC. None of the other statements are true.\nD. There exists a unique $w^*$ which linearly separates our dataset.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Which of the following is true for Generative Adversarial Networks (GANs) but not Diffusion Models:\n\nOptions:\nA. They gradually remove noise from inputs using a Markov chain.\nB. They use a simple $L_2$ loss function.\nC. They can generate new data from input noise.\nD. They use two different models during training.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following transformations to the data matrix $X$ will affect the principal components obtained through PCA?\n\nOptions:\nA. Adding a constant value to all elements of $X$.\nB. None of the other options.\nC. Multiplying one of the features of $X$ by a constant.\nD. Adding an extra feature that is constant across all data points.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: We apply a Gaussian Mixture Model made of $K$ isotropic Gaussians (invariant to rotation around its center) to $N$ vectors of dimension $D$. What is the number of free parameters of this model?\n\nOptions:\nA. $NKD + N$\nB. $2NKD$\nC. $NKD + NKD^2$\nD. $NKD + NKD^2 + N$\nE. $KD + 2K - 1 + N - 1$\nF. $2KD + N - 1$\nG. $NKD$\nH. $2NKD + N$\nI. $KD + KD^2 - 1$\nJ. $KD + K - 1$\nK. $KD + K + N$\nM. $2KD - 1$\nN. $KD + 2K - 1 - 1$\n\nAnswer:", "answer": "N"}
{"subject": "machine_learning", "question": "Question: We define a simplified Gaussian Mixture Model consisting of 2 equally likely Gaussians, i.e. $K = 2$ and $\\pi_1 = \\pi_2 = 0.5$, and covariance matrices of the form $\\Sigma_i = \\sigma_i I_{D \\times D}$ for $i \\in \\{1, 2\\}$ with $I_{D \\times D}$ the identity matrix of size $D$. The dataset consists of only 2 points $x_1$ and $x_2$ that are distinct ($x_1 \\neq x_2$). We initialize the model at some finite $\\mu_1^{(0)}, \\mu_2^{(0)}$ and $\\sigma_1^{(0)}, \\sigma_2^{(0)}$. We fit the model by the EM method on these parameters (keeping $\\pi_1$ and $\\pi_2$ fixed to 0.5). After $T \\rightarrow \\infty$ steps, select the true statement among the following:\n\nOptions:\nA. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ diverge to $\\infty$ for some but not all the initializations.\nB. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ diverge to $\\infty$ for any initializations.\nC. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for any initializations.\nD. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for some but not all the initializations.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Let $Y = XW + b$, where $X,W,Y \\in \\mathbb{R}^{k \\times k}$ and $b \\in \\mathbb{R}^{1 \\times k}$, represent a linear layer of width $k$ operating on a batch of $k$ inputs where the addition is broadcasted as in Numpy or PyTorch. The network is trained with respect to a loss function $L(Y)$ that only depends on $W$ and $b$ through $Y$. Given $\\delta_Y = \\frac{\\partial L}{\\partial Y}$, how can we compute $\\delta_W = \\frac{\\partial L}{\\partial W}$ and $\\delta_b = \\frac{\\partial L}{\\partial b}$? Let $1_{1,k} = [1, 1, . . . , 1]$ with shape $1 \\times k$.\n\nOptions:\nA. $\\delta_W = X \\delta_Y, \\delta_b = 1_{1,k} \\delta_Y^T$\nB. $\\delta_W = X^T \\delta_Y, \\delta_b = 1_{1,k} \\delta_Y$\nC. $\\delta_W = \\delta_Y X, \\delta_b = 1_{1,k} \\delta_Y^T$\nD. $\\delta_W = \\delta_Y X^T, \\delta_b = 1_{1,k} \\delta_Y^T$\nE. $\\delta_W = X \\delta_Y, \\delta_b = 1_{1,k} \\delta_Y$\nF. $\\delta_W = \\delta_Y X^T, \\delta b = 1_{1,k} \\delta_Y$\nG. $\\delta_W = X^T \\delta_Y, \\delta_b = 1_{1,k} \\delta_Y^T$\nH. $\\delta_W = \\delta_Y X, \\delta_b = 1_{1,k} \\delta_Y$\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: A neural network has been trained for multi-class classification using cross-entropy but has not necessarily achieved a global or local minimum on the training set. The output of the neural network is $z = [z_1, . . . , z_d]^T$ obtained from the penultimate values $x = [x_1, . . . , x_d]^T$ via softmax $z_k = \\frac{\\exp(x_k)}{\\sum_i \\exp(x_i)}$ that can be interpreted as a probability distribution over the $d$ possible classes. The cross-entropy is given by $H(y, z) = -\\sum_{i=1}^d y_i \\ln z_i$ where $y$ is one-hot encoded meaning the entity corresponding to the true class is 1 and other entities are 0. We now modify the neural network, either by scaling $x \\rightarrow \\alpha x$ where $\\alpha \\in \\mathbb{R}_{>0}$ or through a shift $x \\rightarrow x + b1$ where $b \\in \\mathbb{R}$. The modified $x$ values are fed into the softmax to obtain the final output and the network parameters are otherwise unchanged. How do these transformations affect the training accuracy of the network?\n\nOptions:\nA. Both transformations sometimes increase and sometimes decrease the accuracy.\nB. Both transformations decrease the accuracy in some cases (but never increase it).\nC. Neither transformation affects the accuracy.\nD. One transformation has no effect, the other sometimes increases and sometimes decreases the accuracy.\nE. One transformation has no effect, the other one decreases the accuracy in some cases (but never increases it).\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Consider a 2D convolutional layer with a kernel size of $3 \\times 3$, 10 input channels and 10 output channels. The input to the convolution is a tensor $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ with batch size $N = 1$, number of channels $C = 10$, height $H = 10$ and width $W = 10$. We use a zero padding of 1 on each side and obtain an output $Y$ of the same shape as $X$. If we flatten the input and output into vectors $x,y \\in \\mathbb{R}^{CHW}$ respectively, we can compute $y = Ax$ for a matrix $A$ that depends on the original convolutional weights. What is the sparsity $\\zeta$ of $A$, assuming the original convolutional weights contain no zeros? Note that the sparsity of a matrix is defined as the fraction (or percentage) of its elements that are zeros.\n\nOptions:\nA. $91.5\\% < \\zeta \\leq 93.0\\%$\nB. $90.0\\% < \\zeta \\leq 91.5\\%$\nC. $\\zeta > 96.0\\%$\nD. $94.5\\% < \\zeta \\leq 96.0\\%$\nE. $93.0\\% < \\zeta \\leq 94.5\\%$\nF. $\\zeta \\leq 90.0\\%$\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Let $x,w, \\delta \\in \\mathbb{R}^d$, $y \\in \\{-1, 1\\}$, and $\\epsilon \\in \\mathbb{R}_{>0}$ be an arbitrary positive value. Which of the following is NOT true in general:\n\nOptions:\nA. $\\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} \\log_2(1 + \\exp(-y w^T (x + \\delta))) = \\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} 1_{y w^T (x + \\delta) \\leq 0}$\nB. $\\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} \\log_2(1 + \\exp(-y w^T (x + \\delta))) = \\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} 1 - \\tanh(y w^T (x + \\delta))$\nC. $\\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} \\log_2(1 + \\exp(-y w^T (x + \\delta))) = \\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} \\exp(-y w^T (x + \\delta))$\nD. $\\operatorname{arg\\,max}_{\\|\\delta\\|_2 \\leq \\epsilon} \\log_2(1 + \\exp(-y w^T (x + \\delta))) = \\operatorname{arg\\,min}_{\\|\\delta\\|_2 \\leq \\epsilon} y w^T (x + \\delta)$\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Which statement about black-box adversarial attacks is true:\n\nOptions:\nA. They require access to the gradients of the model being attacked.\nB. They cannot be implemented via gradient-free (e.g., grid search or random search) optimization methods.\nC. They are highly specific and cannot be transferred from a model which is similar to the one being attacked.\nD. They can be implemented using gradient approximation via a finite difference formula.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Which of the following probability distributions are members of the exponential family:\n\nOptions:\nA. Uniform distribution over $[0, \\eta], \\eta > 0: p(y|\\eta) = \\frac{1}{\\eta} 1_{y \\in [0, \\eta]}$.\nB. Cauchy distribution: $p(y|y_0, \\gamma) = \\frac{1}{\\pi \\gamma [1 + (\\frac{y - y_0}{\\gamma})^2]}$.\nC. Poisson distribution: $p(y|\\mu) = \\frac{e^{-y}}{y!} \\mu^y$.\n\nAnswer:", "answer": "C"}
{"subject": "machine_learning", "question": "Question: Which of the following statements is true about nearest neighbor classifiers:\n\nOptions:\nA. None of the other answers.\nB. Nearest neighbors can be slow to find in high-dimensional spaces.\nC. Nearest neighbor classifiers can only work with the Euclidean distance.\nD. Nearest neighbor classifiers do not need to store the training data.\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Consider a movie recommendation system which minimizes the following objective $\\frac{1}{2} \\sum_{(d,n) \\in \\Omega} [x_{dn} - (WZ^T)_{dn}]^2 + \\frac{\\lambda_w}{2} \\|W\\|_F^2 + \\frac{\\lambda_z}{2} \\|Z\\|_F^2$ where $W \\in \\mathbb{R}^{D \\times K}$ and $Z \\in \\mathbb{R}^{N \\times K}$. Suppose movies are divided into genre A and genre B (i.e., $W_A \\in \\mathbb{R}^{D_A \\times K}$, $W_B \\in \\mathbb{R}^{D_B \\times K}$, $W = [W_A; W_B]$, with $D_A + D_B = D$) and users are divided into group 1 and group 2 (i.e., $Z_1 \\in \\mathbb{R}^{N_1 \\times K}$, $Z_2 \\in \\mathbb{R}^{N_2 \\times K}$, $Z = [Z_1; Z_2]$, with $N_1 + N_2 = N$). In addition, group 1 users only rate genre A movies while group 2 users only rate genre B movies. Then instead of training a large recommendation system with $(W, Z)$, one may train two smaller recommendation systems with parameters $(W_A, Z_1)$ and $(W_B, Z_2)$ separately. If SGD is used to solve the minimization problems and all conditions remain the same (e.g., hyperparameters, sampling order, initialization, etc), then which of the following statements is true about the two training methods?\n\nOptions:\nA. Feature vectors obtained in both cases can be either same or different, depending on the sparsity of the rating matrix.\nB. Feature vectors obtained in both cases remain the same.\nC. Feature vectors obtained in both cases are different.\nD. Feature vectors obtained in both cases can be either same or different, depending on if ratings in two groups and genres are evenly distributed.\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Which of the following statements is incorrect?\n\nOptions:\nA. Word2Vec is based on the idea that a word's meaning is given by the words that frequently appear close-by.\nB. The order of words is ignored in FastText.\nC. Word vectors can be obtained through unsupervised training.\nD. Skip-gram predicts the center word from the bag of context words.\n\nAnswer:", "answer": "D"}
{"subject": "machine_learning", "question": "Question: Consider the function $f : \\mathbb{R} \\rightarrow \\mathbb{R}, f(x) = |x - 2023|$. A subgradient of $f$ at $x = 2023$ exists and is unique.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: In ridge regression, a large regularization parameter $\\lambda$ causes overfitting whereas a small regularization parameter causes underfitting.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: The loss function used in logistic regression equally penalizes positive and negative deviations from the correct class label.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: A model which has a high bias necessarily has a low variance.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Recall that we say that a kernel $K : \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is valid if there exists $k \\in \\mathbb{N}$ and $\\Phi : \\mathbb{R} \\rightarrow \\mathbb{R}^k$ such that for all $(x, x') \\in \\mathbb{R} \\times \\mathbb{R}$, $K(x, x') = \\Phi(x)^T \\Phi(x')$. The kernel $K(x, x') = \\cos(x + x')$ is a valid kernel.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Generative Adversarial Networks use the generator and discriminator models during training but only the discriminator for data synthesis.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Fitting a Gaussian Mixture Model with a single Gaussian $(K = 1)$ will converge after one step of Expectation-Maximization.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Consider two fully connected networks, A and B, with a constant width for all layers, inputs and outputs. Network A has depth $3L$ and width $H$, network B has depth $L$ and width $2H$. Everything else is identical for the two networks and both $L$ and $H$ are large. In this case, performing a single iteration of backpropagation requires fewer scalar multiplications for network A than for network B.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: The output of a 2D convolutional layer with filter size $S \\times S$, where $S \\geq 1$, always has a larger receptive field than the input to the convolution.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: The primal formulation of the soft-margin SVM is NOT equivalent to $\\ell_2$ adversarial training for a linear model trained with the hinge loss $(\\ell(z) = \\max\\{0, 1 - z\\})$.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "A"}
{"subject": "machine_learning", "question": "Question: Nearest neighbor classifiers cannot be used for regression because they rely on majority voting, which is not suited for continuous labels.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
{"subject": "machine_learning", "question": "Question: Antonyms, such as 'hot' and 'cold', have very different context words.\n\nOptions:\nA. True\nB. False\n\nAnswer:", "answer": "B"}
